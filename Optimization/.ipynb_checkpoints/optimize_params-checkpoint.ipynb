{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "actual-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPool1D\n",
    "from keras.initializers import random_uniform\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler as SS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding,Dense,LSTM,Dropout,Flatten,BatchNormalization,Conv1D,GlobalMaxPooling1D,MaxPooling1D\n",
    "from keras.optimizers import  SGD\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import SimpleRNN\n",
    "#from hyperas.distributions import uniform\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import warnings\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-northwest",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attempted-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'...') # Use your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "touched-gasoline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2425727, 79)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tender-quantity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2425727, 78), (2425727,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting dataset into features and labels.\n",
    "\n",
    "labels = dataset['Label']\n",
    "features = dataset.loc[:, dataset.columns != 'Label'].astype('float64')\n",
    "features.shape , labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceramic-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Robust Scalar to scale data \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parliamentary-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "scaler.fit(features)\n",
    "features = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "guilty-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.40474359e+02, -1.08800488e-02,  0.00000000e+00, -5.00000000e-01,\n",
       "       -1.73374613e-01, -1.01886792e-01, -1.57657658e-01,  1.08108108e-01,\n",
       "       -6.59574468e-01,  0.00000000e+00, -1.11356119e-01,  0.00000000e+00,\n",
       "       -4.00833491e-01,  0.00000000e+00,  6.24025254e+01,  4.40008275e+01,\n",
       "       -3.54918620e-02, -8.36254323e-03, -9.42871935e-03, -1.47058824e-02,\n",
       "       -8.75642074e-06, -5.47273279e-05,  0.00000000e+00, -9.27606653e-06,\n",
       "        0.00000000e+00, -1.75808720e-05, -9.01237850e-05,  0.00000000e+00,\n",
       "       -2.23633599e-05, -6.38297872e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -2.14285714e-01, -3.57142857e-01,\n",
       "        7.96727544e+01, -8.97344275e-02,  1.08108108e-01, -7.19629498e-02,\n",
       "       -3.65960885e-01, -9.30566480e-02, -8.52278784e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.00000000e+00,\n",
       "       -4.14028384e-01, -6.59574468e-01, -4.00833491e-01, -2.14285714e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.73374613e-01,\n",
       "       -5.00000000e-01, -1.01886792e-01, -2.66080801e-02, -4.23728814e-03,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if scaling was done\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suitable-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "narrow-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "seasonal-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelEncoder = LabelEncoder()\n",
    "LabelEncoder.fit(labels)\n",
    "labels = LabelEncoder.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "constitutional-antique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing the labels with integers\n",
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "editorial-arctic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BENIGN', 'DDoS', 'PortScan', 'Bot', 'Infiltration',\n",
       "       'Web_Attack_Brute_Force', 'Web_Attack_XSS',\n",
       "       'Web_Attack_Sql_Injection', 'FTPPatator', 'SSHPatator',\n",
       "       'DoS_slowloris', 'DoS_Slowhttptest', 'DoS_Hulk', 'DoS_GoldenEye',\n",
       "       'Heartbleed'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if reverse encoding works\n",
    "check = LabelEncoder.inverse_transform(labels)\n",
    "check = pd.Series(check)\n",
    "check.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "yellow-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "innovative-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\n",
    "    \"train_logs\",\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "\n",
    "# TF callback that sets up TensorBoard with training logs.\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# TF callback that stops training when best value of validationi loss function is reached. It also\n",
    "# restores weights from the best training iteration.\n",
    "eary_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "serial-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "noble-photography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1940581, 78), (485146, 78), (1940581,), (485146,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next step is to split training and testing data. For this we will use sklearn function train_test_split().\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=.2)\n",
    "\n",
    "features_train.shape, features_test.shape, labels_train.shape, labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "divided-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(prediction, y_true):\n",
    "    print('\\n        ----------Validation Data------------------')\n",
    "    print('Accuarcy:',accuracy_score(y_true, prediction)* 100)\n",
    "    print('Precision: {:,.2f} %'.format(precision_score(y_true, prediction, average='weighted') * 100))\n",
    "    print('Recall-score: {:,.2f}'.format(recall_score(y_true, prediction, average='weighted') * 100))\n",
    "    print('F1-score: {:,.2f}'.format(f1_score(y_true, prediction, average='weighted') * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-utilization",
   "metadata": {},
   "source": [
    "### Finding optimal model parameters for DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "parallel-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_layers = [ 1 , 2 ,3 ]\n",
    "layer_sizes  = [64, 128 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "emerging-blast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deep layers-1-Number of nodes-64.\n",
      "Number of deep layers-1-Number of nodes-128.\n",
      "Number of deep layers-2-Number of nodes-64.\n",
      "Number of deep layers-2-Number of nodes-128.\n",
      "Number of deep layers-3-Number of nodes-64.\n",
      "Number of deep layers-3-Number of nodes-128.\n"
     ]
    }
   ],
   "source": [
    "# Number of models to run:\n",
    "for deep_layer in deep_layers :\n",
    "    for layer_size in layer_sizes :\n",
    "        # Print model \n",
    "        NAME =\"Number of deep layers-{}-Number of nodes-{}.\".format(deep_layer, layer_size)\n",
    "        print(NAME)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "moving-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data helper function :\n",
    "def report(prediction, y_true):\n",
    "    print('\\n        ----------Validation Data------------------')\n",
    "    print('Accuarcy:',accuracy_score(y_true, prediction)* 100)\n",
    "    print('Precision: {:,.2f} %'.format(precision_score(y_true, prediction, average='weighted') * 100))\n",
    "    print('Recall-score: {:,.2f}'.format(recall_score(y_true, prediction, average='weighted') * 100))\n",
    "    print('F1-score: {:,.2f}'.format(f1_score(y_true, prediction, average='weighted') * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerical-protest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "30322/30322 [==============================] - 61s 2ms/step - loss: 8822.2850 - accuracy: 0.8664\n",
      "Epoch 2/25\n",
      "30322/30322 [==============================] - 61s 2ms/step - loss: 5.0851 - accuracy: 0.9063\n",
      "Epoch 3/25\n",
      "30322/30322 [==============================] - 64s 2ms/step - loss: 24.8020 - accuracy: 0.9065\n",
      "Epoch 4/25\n",
      "30322/30322 [==============================] - 69s 2ms/step - loss: 6.9596 - accuracy: 0.9070\n",
      "Epoch 5/25\n",
      "30322/30322 [==============================] - 71s 2ms/step - loss: 4.9479 - accuracy: 0.9064\n",
      "Epoch 6/25\n",
      "30322/30322 [==============================] - 74s 2ms/step - loss: 9.9811 - accuracy: 0.9064\n",
      "Epoch 7/25\n",
      "30322/30322 [==============================] - 71s 2ms/step - loss: 10.2214 - accuracy: 0.9070\n",
      "Epoch 8/25\n",
      "30322/30322 [==============================] - 74s 2ms/step - loss: 9.2303 - accuracy: 0.9071\n",
      "Epoch 9/25\n",
      "30322/30322 [==============================] - 71s 2ms/step - loss: 20.5187 - accuracy: 0.9064\n",
      "Epoch 10/25\n",
      "30322/30322 [==============================] - 68s 2ms/step - loss: 19.2551 - accuracy: 0.9066\n",
      "Epoch 11/25\n",
      "30322/30322 [==============================] - 67s 2ms/step - loss: 15.4118 - accuracy: 0.9060\n",
      "Epoch 12/25\n",
      "30322/30322 [==============================] - ETA: 0s - loss: 11.8155 - accuracy: 0.905 - 68s 2ms/step - loss: 11.8165 - accuracy: 0.9053\n",
      "Results for model : \n",
      "Number of deep layers-1-Number of nodes-64.\n",
      "\n",
      "        ----------Validation Data------------------\n",
      "Accuarcy: 90.81183808585457\n",
      "Precision: 91.49 %\n",
      "Recall-score: 90.81\n",
      "F1-score: 88.10\n",
      "Epoch 1/25\n",
      "30322/30322 [==============================] - 96s 3ms/step - loss: 9805.7016 - accuracy: 0.8757\n",
      "Epoch 2/25\n",
      "30322/30322 [==============================] - 91s 3ms/step - loss: 10.4064 - accuracy: 0.9061\n",
      "Epoch 3/25\n",
      "30322/30322 [==============================] - 93s 3ms/step - loss: 19.9073 - accuracy: 0.9072\n",
      "Epoch 4/25\n",
      "30322/30322 [==============================] - 98s 3ms/step - loss: 25.2817 - accuracy: 0.9070\n",
      "Epoch 5/25\n",
      "30322/30322 [==============================] - 91s 3ms/step - loss: 37.1733 - accuracy: 0.9068\n",
      "Epoch 6/25\n",
      "30322/30322 [==============================] - 86s 3ms/step - loss: 36.6947 - accuracy: 0.9074\n",
      "Epoch 7/25\n",
      "30322/30322 [==============================] - 87s 3ms/step - loss: 50.3242 - accuracy: 0.9069\n",
      "Epoch 8/25\n",
      "30322/30322 [==============================] - 82s 3ms/step - loss: 35.6050 - accuracy: 0.9071\n",
      "Epoch 9/25\n",
      "30322/30322 [==============================] - 83s 3ms/step - loss: 12.8965 - accuracy: 0.9074\n",
      "Epoch 10/25\n",
      "30322/30322 [==============================] - 82s 3ms/step - loss: 44.1247 - accuracy: 0.9073\n",
      "Epoch 11/25\n",
      "30322/30322 [==============================] - 79s 3ms/step - loss: 52.5174 - accuracy: 0.9068\n",
      "Epoch 12/25\n",
      "30322/30322 [==============================] - 80s 3ms/step - loss: 95.1720 - accuracy: 0.9071\n",
      "Epoch 13/25\n",
      "30322/30322 [==============================] - 80s 3ms/step - loss: 208.8573 - accuracy: 0.9070\n",
      "Epoch 14/25\n",
      "30322/30322 [==============================] - 77s 3ms/step - loss: 6.8520 - accuracy: 0.9069\n",
      "Epoch 15/25\n",
      "30322/30322 [==============================] - 77s 3ms/step - loss: 41.6304 - accuracy: 0.9067\n",
      "Epoch 16/25\n",
      "30322/30322 [==============================] - 77s 3ms/step - loss: 103.3949 - accuracy: 0.9068\n",
      "Epoch 17/25\n",
      "30322/30322 [==============================] - 79s 3ms/step - loss: 65.6387 - accuracy: 0.9071\n",
      "Epoch 18/25\n",
      "30322/30322 [==============================] - 78s 3ms/step - loss: 15.0836 - accuracy: 0.9070\n",
      "Epoch 19/25\n",
      "30322/30322 [==============================] - 76s 3ms/step - loss: 241.7167 - accuracy: 0.9073\n",
      "Epoch 20/25\n",
      "30322/30322 [==============================] - 77s 3ms/step - loss: 18.4621 - accuracy: 0.9069\n",
      "Epoch 21/25\n",
      "30322/30322 [==============================] - 77s 3ms/step - loss: 9.1731 - accuracy: 0.9065\n",
      "Epoch 22/25\n",
      "30322/30322 [==============================] - 79s 3ms/step - loss: 56.8043 - accuracy: 0.9075\n",
      "Epoch 23/25\n",
      "30322/30322 [==============================] - 78s 3ms/step - loss: 3.2404 - accuracy: 0.9071\n",
      "Epoch 24/25\n",
      "30322/30322 [==============================] - 77s 3ms/step - loss: 5.0357 - accuracy: 0.9068\n",
      "Epoch 25/25\n",
      "30322/30322 [==============================] - 77s 3ms/step - loss: 0.9301 - accuracy: 0.9068\n",
      "Results for model : \n",
      "Number of deep layers-1-Number of nodes-128.\n",
      "\n",
      "        ----------Validation Data------------------\n",
      "Accuarcy: 90.77762158195677\n",
      "Precision: 91.62 %\n",
      "Recall-score: 90.78\n",
      "F1-score: 88.06\n",
      "Epoch 1/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 20843.7428 - accuracy: 0.8501\n",
      "Epoch 2/25\n",
      "15161/15161 [==============================] - 41s 3ms/step - loss: 3.2227 - accuracy: 0.9012\n",
      "Epoch 3/25\n",
      "15161/15161 [==============================] - 40s 3ms/step - loss: 2.3956 - accuracy: 0.9070\n",
      "Epoch 4/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 5.1863 - accuracy: 0.9076\n",
      "Epoch 5/25\n",
      "15161/15161 [==============================] - 36s 2ms/step - loss: 1.0990 - accuracy: 0.9072\n",
      "Epoch 6/25\n",
      "15161/15161 [==============================] - 36s 2ms/step - loss: 2.2090 - accuracy: 0.9078\n",
      "Epoch 7/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 5.8481 - accuracy: 0.9077\n",
      "Epoch 8/25\n",
      "15161/15161 [==============================] - 38s 2ms/step - loss: 4.3565 - accuracy: 0.9068\n",
      "Epoch 9/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 3.6487 - accuracy: 0.9069\n",
      "Epoch 10/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 2.6091 - accuracy: 0.9080\n",
      "Epoch 11/25\n",
      "15161/15161 [==============================] - 38s 2ms/step - loss: 1.7876 - accuracy: 0.9077\n",
      "Epoch 12/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 4.6262 - accuracy: 0.9070\n",
      "Epoch 13/25\n",
      "15161/15161 [==============================] - 36s 2ms/step - loss: 0.9682 - accuracy: 0.9077\n",
      "Epoch 14/25\n",
      "15161/15161 [==============================] - 36s 2ms/step - loss: 2.9033 - accuracy: 0.9078\n",
      "Epoch 15/25\n",
      "15161/15161 [==============================] - 38s 3ms/step - loss: 2.5757 - accuracy: 0.9078\n",
      "Results for model : \n",
      "Number of deep layers-2-Number of nodes-64.\n",
      "\n",
      "        ----------Validation Data------------------\n",
      "Accuarcy: 90.85471177748555\n",
      "Precision: 91.62 %\n",
      "Recall-score: 90.85\n",
      "F1-score: 88.18\n",
      "Epoch 1/25\n",
      "15161/15161 [==============================] - 47s 3ms/step - loss: 12385.2183 - accuracy: 0.8693\n",
      "Epoch 2/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 0.9145 - accuracy: 0.9053\n",
      "Epoch 3/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 7.1001 - accuracy: 0.9068\n",
      "Epoch 4/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 4.4456 - accuracy: 0.9073\n",
      "Epoch 5/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 7.2843 - accuracy: 0.9071\n",
      "Epoch 6/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 5.9429 - accuracy: 0.9076\n",
      "Epoch 7/25\n",
      "15161/15161 [==============================] - 47s 3ms/step - loss: 17.2304 - accuracy: 0.9076\n",
      "Epoch 8/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 6.9685 - accuracy: 0.9078\n",
      "Epoch 9/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 27.2471 - accuracy: 0.9074\n",
      "Epoch 10/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 9.4955 - accuracy: 0.9077\n",
      "Epoch 11/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 34.5350 - accuracy: 0.9070\n",
      "Epoch 12/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 22.0430 - accuracy: 0.9073\n",
      "Results for model : \n",
      "Number of deep layers-2-Number of nodes-128.\n",
      "\n",
      "        ----------Validation Data------------------\n",
      "Accuarcy: 90.79658494556277\n",
      "Precision: 91.53 %\n",
      "Recall-score: 90.80\n",
      "F1-score: 88.08\n",
      "Epoch 1/25\n",
      "15161/15161 [==============================] - 38s 2ms/step - loss: 15082.0896 - accuracy: 0.8450\n",
      "Epoch 2/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 1.1551 - accuracy: 0.9017\n",
      "Epoch 3/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 1.6005 - accuracy: 0.9071\n",
      "Epoch 4/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 2.0201 - accuracy: 0.9076\n",
      "Epoch 5/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 3.6022 - accuracy: 0.9069\n",
      "Epoch 6/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 1.3977 - accuracy: 0.9075\n",
      "Epoch 7/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 4.9971 - accuracy: 0.9075\n",
      "Epoch 8/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 2.9780 - accuracy: 0.9074\n",
      "Epoch 9/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 3.9189 - accuracy: 0.9073\n",
      "Epoch 10/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 5.6960 - accuracy: 0.9073\n",
      "Epoch 11/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 8.3175 - accuracy: 0.9066\n",
      "Epoch 12/25\n",
      "15161/15161 [==============================] - 37s 2ms/step - loss: 2.6926 - accuracy: 0.9072\n",
      "Results for model : \n",
      "Number of deep layers-3-Number of nodes-64.\n",
      "\n",
      "        ----------Validation Data------------------\n",
      "Accuarcy: 90.78998899300417\n",
      "Precision: 91.27 %\n",
      "Recall-score: 90.79\n",
      "F1-score: 88.07\n",
      "Epoch 1/25\n",
      "15161/15161 [==============================] - 45s 3ms/step - loss: 8643.5688 - accuracy: 0.8609\n",
      "Epoch 2/25\n",
      "15161/15161 [==============================] - 44s 3ms/step - loss: 2.4203 - accuracy: 0.9059\n",
      "Epoch 3/25\n",
      "15161/15161 [==============================] - 44s 3ms/step - loss: 9.7006 - accuracy: 0.9072\n",
      "Epoch 4/25\n",
      "15161/15161 [==============================] - 44s 3ms/step - loss: 23.2219 - accuracy: 0.9075\n",
      "Epoch 5/25\n",
      "15161/15161 [==============================] - 45s 3ms/step - loss: 7.2735 - accuracy: 0.9077\n",
      "Epoch 6/25\n",
      "15161/15161 [==============================] - 46s 3ms/step - loss: 27.7551 - accuracy: 0.9076\n",
      "Epoch 7/25\n",
      "15161/15161 [==============================] - 45s 3ms/step - loss: 16.7339 - accuracy: 0.9080\n",
      "Epoch 8/25\n",
      "15161/15161 [==============================] - 45s 3ms/step - loss: 22.9885 - accuracy: 0.9078\n",
      "Epoch 9/25\n",
      "15161/15161 [==============================] - 45s 3ms/step - loss: 34.7911 - accuracy: 0.9075\n",
      "Epoch 10/25\n",
      "15161/15161 [==============================] - 45s 3ms/step - loss: 8.3581 - accuracy: 0.9075\n",
      "Epoch 11/25\n",
      "15161/15161 [==============================] - 45s 3ms/step - loss: 24.2222 - accuracy: 0.9078\n",
      "Epoch 12/25\n",
      "15161/15161 [==============================] - 45s 3ms/step - loss: 11.2248 - accuracy: 0.9078\n",
      "Results for model : \n",
      "Number of deep layers-3-Number of nodes-128.\n",
      "\n",
      "        ----------Validation Data------------------\n",
      "Accuarcy: 90.73351114922106\n",
      "Precision: 91.42 %\n",
      "Recall-score: 90.73\n",
      "F1-score: 88.05\n"
     ]
    }
   ],
   "source": [
    "for deep_layer in deep_layers :\n",
    "    for layer_size in layer_sizes :\n",
    "        \n",
    "        model = Sequential()\n",
    "            \n",
    "        model.add(Dense(layer_size ,input_dim=78,activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        for l in range(len(deep_layers) - 1):\n",
    "            model.add(Dense(layer_size,activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "                \n",
    "        model.add(Dense(15, activation='softmax'))\n",
    "                          \n",
    "        model.compile(loss ='sparse_categorical_crossentropy', optimizer= 'adam', metrics = ['accuracy'])\n",
    "            \n",
    "        if deep_layer == 1 :\n",
    "            batch_size_value = 64\n",
    "        else :\n",
    "            batch_size_value = 128\n",
    "            \n",
    "        model.fit(features_train,labels_train,epochs=25,batch_size = batch_size_value,callbacks=[tensorboard_callback, eary_stop_callback])\n",
    "            \n",
    "        # Print model\n",
    "        print(\"Results for model : \")\n",
    "        NAME =\"Number of deep layers-{}-Number of nodes-{}.\".format(deep_layer, layer_size,int(time.time()))\n",
    "        print(NAME)\n",
    "            \n",
    "        # Evaluating model accuracy.\n",
    "        predict = np.argmax(model.predict(features_test),axis=1)\n",
    "        a = np.unique(predict)\n",
    "        b = np.unique(labels_test)\n",
    "        c = list(set(a) | set(b))\n",
    "        report(predict,labels_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-framing",
   "metadata": {},
   "source": [
    "### Finding optimal model parameters for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "limited-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dense_layers =[ 1, 2, 3,]\n",
    "cnn_conv_layers = [ 1 , 2 ,3 ]\n",
    "cnn_layer_sizes  = [64, 128 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "compressed-collaboration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conv layers-1-Number of nodes-64-number of dense layers-1.\n",
      "Number of conv layers-2-Number of nodes-64-number of dense layers-1.\n",
      "Number of conv layers-3-Number of nodes-64-number of dense layers-1.\n",
      "Number of conv layers-1-Number of nodes-128-number of dense layers-1.\n",
      "Number of conv layers-2-Number of nodes-128-number of dense layers-1.\n",
      "Number of conv layers-3-Number of nodes-128-number of dense layers-1.\n",
      "Number of conv layers-1-Number of nodes-64-number of dense layers-2.\n",
      "Number of conv layers-2-Number of nodes-64-number of dense layers-2.\n",
      "Number of conv layers-3-Number of nodes-64-number of dense layers-2.\n",
      "Number of conv layers-1-Number of nodes-128-number of dense layers-2.\n",
      "Number of conv layers-2-Number of nodes-128-number of dense layers-2.\n",
      "Number of conv layers-3-Number of nodes-128-number of dense layers-2.\n",
      "Number of conv layers-1-Number of nodes-64-number of dense layers-3.\n",
      "Number of conv layers-2-Number of nodes-64-number of dense layers-3.\n",
      "Number of conv layers-3-Number of nodes-64-number of dense layers-3.\n",
      "Number of conv layers-1-Number of nodes-128-number of dense layers-3.\n",
      "Number of conv layers-2-Number of nodes-128-number of dense layers-3.\n",
      "Number of conv layers-3-Number of nodes-128-number of dense layers-3.\n"
     ]
    }
   ],
   "source": [
    "# Number of models to run:\n",
    "for cnn_dense_layer in cnn_dense_layers:\n",
    "    for cnn_layer_size in cnn_layer_sizes :\n",
    "        for cnn_conv_layer in cnn_conv_layers :\n",
    "            # Print model \n",
    "            NAME =\"Number of conv layers-{}-Number of nodes-{}-number of dense layers-{}.\".format(cnn_conv_layer, cnn_layer_size,cnn_dense_layer,int(time.time()))\n",
    "            print(NAME)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "present-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= to_categorical(labels_train)\n",
    "y_test= to_categorical(labels_test)\n",
    "X_train = np.array(features_train).reshape(features_train.shape[0], features_train.shape[1], 1)\n",
    "X_test = np.array(features_test).reshape(features_test.shape[0], features_test.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "intermediate-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1040941203\n",
    "hidden_initializer = random_uniform(seed=SEED)\n",
    "\n",
    "# Number of models to run:\n",
    "for cnn_dense_layer in cnn_dense_layers:\n",
    "    for cnn_layer_size in cnn_layer_sizes :\n",
    "        for cnn_conv_layer in cnn_conv_layers :\n",
    "            # Print model \n",
    "            NAME =\"Number of conv layers-{}-Number of nodes-{}-number of dense layers-{}.\".format(cnn_conv_layer, cnn_layer_size,cnn_dense_layer,int(time.time()))\n",
    "            print(NAME)\n",
    "             \n",
    "            model = Sequential()\n",
    "            \n",
    "            model.add(Conv1D(cnn_layer_size ,3,input_shape=(78, 1)))\n",
    "            model.add(LeakyReLU(alpha=0.1))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "        for l in range(cnn_conv_layer - 1):\n",
    "            model.add(Conv1D(cnn_layer_size ,3))\n",
    "            model.add(LeakyReLU(alpha=0.1))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(Dropout(0.4))\n",
    "\n",
    "                \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        for l in range(cnn_dense_layer):\n",
    "            model.add(Dense(cnn_layer_size, input_dim=78, kernel_initializer=hidden_initializer))\n",
    "            model.add(LeakyReLU(alpha=0.1))\n",
    "            \n",
    "            \n",
    "        model.add(Dense(15, activation='softmax'))\n",
    "                          \n",
    "        model.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics = ['accuracy'])\n",
    "            \n",
    "        if deep_layer == 1 :\n",
    "            batch_size_value = 1024\n",
    "        else :\n",
    "            batch_size_value = 2048\n",
    "            \n",
    "        model.fit(X_train,y_train,epochs=25,batch_size = batch_size_value,callbacks=[tensorboard_callback, eary_stop_callback])\n",
    "            \n",
    "        # Print model \n",
    "        NAME =\"Number of conv layers-{}-Number of nodes-{}-number of dense layers-{}.\".format(cnn_conv_layer, cnn_layer_size,cnn_dense_layer,int(time.time()))\n",
    "        print(NAME)\n",
    "            \n",
    "        # Evaluating model accuracy.\n",
    "        predict = np.argmax(model.predict(X_test),axis=1)\n",
    "        a = np.unique(predict)\n",
    "        b = np.unique(y_test)\n",
    "        c = list(set(a) | set(b))\n",
    "        report(predict,y_test)\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-thursday",
   "metadata": {},
   "source": [
    "### Finding optimal model parameters for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_dense_layers =[ 1, 2, 3,]\n",
    "rnn_conv_layers = [ 1 , 2 ,3 ]\n",
    "rnn_layer_sizes  = [32 ,64 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features] for RNN\n",
    "RNN_features_train = np.reshape(features_train, (features_train.shape[0],features_train.shape[1],1))\n",
    "RNN_features_test = np.reshape(features_test, (features_test.shape[0],features_test.shape[1],1))\n",
    "RNN_features_val = np.reshape(features_val, (features_val.shape[0],features_val.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1040941203\n",
    "hidden_initializer = random_uniform(seed=SEED)\n",
    "\n",
    "# Number of models to run:\n",
    "for rnn_dense_layer in cnn_dense_layers:\n",
    "    for rnn_layer_size in cnn_layer_sizes :\n",
    "        for rnn_layer in cnn_layers :\n",
    "            # Print model \n",
    "            NAME =\"Number of conv layers-{}-Number of nodes-{}-number of dense layers-{}.\".format(rnn_layer, rnn_layer_size,rnn_dense_layer,int(time.time()))\n",
    "            print(NAME)\n",
    "             \n",
    "            model = Sequential()\n",
    "            \n",
    "            model.add(SimpleRNN(cnn_layer_size, activation='relu', return_sequences = True,  input_shape = (78,1)))\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "        for l in range(rnn_layer - 1):\n",
    "            model.add(SimpleRNN(cnn_layer_size ,activation='relu', return_sequences = True))\n",
    "            model.add(Dropout(0.1))\n",
    "\n",
    "        for l in range(rnn_dense_layer):\n",
    "            model.add(SimpleRNN(rnn_dense_layer,activation='relu', return_sequences = True))\n",
    "            model.add(Dropout(0.1)) \n",
    "\n",
    "        model.add(Flatten())   \n",
    "        model.add(Dense(32, input_dim=78, kernel_initializer=hidden_initializer))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(16))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dense(15, activation='softmax'))\n",
    "                          \n",
    "        model.compile(loss ='categorical_crossentropy', optimizer= 'adam', metrics = ['accuracy'])\n",
    "            \n",
    "        if deep_layer == 1 :\n",
    "            batch_size_value = 1024\n",
    "        else :\n",
    "            batch_size_value = 2048\n",
    "            \n",
    "        model.fit(RNN_features_train,y_train,epochs=25,batch_size = batch_size_value,callbacks=[tensorboard_callback, eary_stop_callback])\n",
    "            \n",
    "        # Print model \n",
    "        NAME =\"Number of conv layers-{}-Number of nodes-{}-number of dense layers-{}.\".format(rnn_layer, rnn_layer_size,rnn_dense_layer,int(time.time()))\n",
    "        print(NAME)\n",
    "            \n",
    "        # Evaluating model accuracy.\n",
    "        predict = np.argmax(model.predict(RNN_features_test),axis=1)\n",
    "        a = np.unique(predict)\n",
    "        b = np.unique(y_test)\n",
    "        c = list(set(a) | set(b))\n",
    "        report(predict,y_test)\n",
    "            \n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
